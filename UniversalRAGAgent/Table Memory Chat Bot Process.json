{
  "name": "Table Memory Chat Bot Process",
  "nodes": [
    {
      "parameters": {
        "promptType": "define",
        "text": "=Analyze if this conversation contains information worth remembering long-term.\n\nCurrent context:\n={{ $json.context }}\n\nLatest exchange:\nUser: {{ $json.userMessage }}\nAssistant: {{ $json.assistantResponse }}\n\nRespond with ONLY valid JSON:\n{\"should_update\": true, \"new_memories\": [{\"type\": \"preference\", \"content\": \"User likes pizza\"}], \"remove_indices\": []}\n\nOnly create memories for important lasting information (preferences, facts, goals). Not for casual greetings.",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.agent",
      "typeVersion": 2.2,
      "position": [
        -1952,
        336
      ],
      "id": "27faa1e6-2a24-4bb7-a4dd-d701c25afd79",
      "name": "Memory Manager"
    },
    {
      "parameters": {
        "jsCode": "// Build context with all summaries (including gap-filled)\nconst trigger = $('Start').first().json || {};\nconst mergedData = $('Merge Gap Summary').first().json;\nconst transcriptRows = mergedData.transcript_rows || [];\nconst summaryRows = mergedData.summary_rows || [];\n\n// Sort transcript\nconst rows = transcriptRows\n  .filter(r => !r.context_id || String(r.context_id) === String(trigger.context_id))\n  .sort((a, b) => {\n    const ts = v => Date.parse(v || '') || 0;\n    return ts(a.created_at || a.createdAt) - ts(b.created_at || b.createdAt);\n  });\n\n// Format recent messages\nconst recent_messages = rows.map(r => {\n  const message = String(r.message ?? r.message_original ?? '').trim();\n  const original = String(r.message_original ?? '').trim();\n  \n  return {\n    id: r.id,\n    role: r.role === 'assistant' ? 'assistant' : 'user',\n    content: message,\n    was_truncated: original.length > 0 && message !== original\n  };\n});\n\n// Format summaries (oldest to newest)\nconst topic_summaries = summaryRows.map(s => ({\n  topic_title: s.topic_title || '',\n  summary: s.topic_summary || '',\n  message_range: [s.first_message_id, s.last_message_id],\n  message_count: s.message_count || 0\n}));\n\n// Get long-term memories\nlet long_term_memories = [];\ntry {\n  if (trigger.context && typeof trigger.context === 'string') {\n    const parsed = JSON.parse(trigger.context);\n    if (Array.isArray(parsed?.long_term_memories)) {\n      long_term_memories = parsed.long_term_memories;\n    }\n  } else if (trigger.context && typeof trigger.context === 'object' &&\n             Array.isArray(trigger.context.long_term_memories)) {\n    long_term_memories = trigger.context.long_term_memories;\n  }\n} catch { /* ignore */ }\n\nconst context = { \n  topic_summaries,\n  recent_messages, \n  long_term_memories \n};\n\nreturn [{\n  json: {\n    context_id: String(trigger.context_id || ''),\n    context: JSON.stringify(context),\n    userMessage: String(trigger.userMessage ?? ''),\n    assistantResponse: String(trigger.assistantResponse ?? '')\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        128,
        240
      ],
      "id": "0b702d5c-5498-448e-8dff-6cd7da6c7ae1",
      "name": "Build Next Context"
    },
    {
      "parameters": {
        "jsCode": "// Configuration\nconst MESSAGE_LENGTH_THRESHOLD = 5000;\nconst TRUNCATE_LENGTH = 1000;\nconst TRUNCATION_SUFFIX = '... [truncated] Message Summary: ';\n\n// Get input data from trigger\nconst triggerData = $('Start').first().json;\nconst contextId = triggerData.context_id;\nconst userMessage = triggerData.userMessage;\nconst assistantResponse = triggerData.assistantResponse;\nconst tablePrefix = triggerData.table_prefix;\nconst contextStr = triggerData.context;\n\n// Process user message\nlet userItem = null;\nif (typeof userMessage === 'string' && userMessage.trim()) {\n  const originalMessage = userMessage.trim();\n  const messageLength = originalMessage.length;\n  \n  userItem = {\n    context_id: contextId,\n    table_prefix: tablePrefix,\n    context: contextStr,\n    role: 'user',\n    message_original: originalMessage,\n    needs_summary: messageLength > MESSAGE_LENGTH_THRESHOLD,\n    truncated_message: messageLength > MESSAGE_LENGTH_THRESHOLD \n      ? originalMessage.substring(0, TRUNCATE_LENGTH) + TRUNCATION_SUFFIX\n      : null,\n    message: messageLength <= MESSAGE_LENGTH_THRESHOLD ? originalMessage : null\n  };\n}\n\n// Process assistant message\nlet assistantItem = null;\nif (typeof assistantResponse === 'string' && assistantResponse.trim()) {\n  const originalMessage = assistantResponse.trim();\n  const messageLength = originalMessage.length;\n  \n  assistantItem = {\n    context_id: contextId,\n    table_prefix: tablePrefix,\n    context: contextStr,\n    role: 'assistant',\n    message_original: originalMessage,\n    needs_summary: messageLength > MESSAGE_LENGTH_THRESHOLD,\n    truncated_message: messageLength > MESSAGE_LENGTH_THRESHOLD \n      ? originalMessage.substring(0, TRUNCATE_LENGTH) + TRUNCATION_SUFFIX\n      : null,\n    message: messageLength <= MESSAGE_LENGTH_THRESHOLD ? originalMessage : null\n  };\n}\n\n// Return items for processing\nconst items = [];\nif (userItem) items.push({ json: userItem });\nif (assistantItem) items.push({ json: assistantItem });\n\nreturn items;"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -1600,
        336
      ],
      "id": "9ba5f708-5a1a-4703-91f7-b877c2aec493",
      "name": "Prepare Messages"
    },
    {
      "parameters": {
        "conditions": {
          "boolean": [
            {
              "value1": "={{ $json.needs_summary }}",
              "value2": true
            }
          ]
        }
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 1,
      "position": [
        -1392,
        336
      ],
      "id": "758dfb67-8d1e-4d9a-b19f-8dc39a298045",
      "name": "Check If Summary Needed"
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "=Create a concise summary of the following message. The summary MUST be 1000 characters or less. Focus on the key points and main ideas.\n\nOriginal message:\n{{ $json.message_original }}\n\nProvide ONLY the summary text, no additional formatting or explanation.",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.agent",
      "typeVersion": 2.2,
      "position": [
        -1184,
        144
      ],
      "id": "19d0901f-b2de-41e0-9e11-741358f9c141",
      "name": "Summarize Long Message"
    },
    {
      "parameters": {
        "jsCode": "// === Combine Message with Summary (per item) ===\n\n// Direct agent summary from previous node\nconst summary = String($input.item?.json?.output ?? '').trim();\n\n// Nearby items on the same branch (if present)\nconst paired = (name) => { try { return ($item(name, 0)?.json ?? {}); } catch { return {}; } };\nconst cur    = $json ?? {};\nconst merged = paired('Merge Messages');            // rename if needed\nconst prep   = paired('Prepare Transcript Rows');   // rename if needed\n\n// Role: from the current item first\nconst role = String(cur.role ?? merged.role ?? prep.role ?? 'user').trim();\n\n// Trigger payload (never used for role, only as last-resort text)\nconst trigger = $('Start').first().json || {};\nconst triggerMessage =\n  role === 'assistant'\n    ? String(trigger.assistantResponse ?? '')\n    : String(trigger.userMessage ?? '');\n\n// Original message: current item -> nearby items -> trigger (role-aware)\nconst originalRaw =\n  cur.message_original ?? cur.message ??\n  merged.message_original ?? merged.message ??\n  prep.message_original ?? prep.message ??\n  triggerMessage;\n\nconst original = String(originalRaw ?? '').trim();\n\n// Minimal required meta\nconst context_id   = String(cur.context_id   ?? merged.context_id   ?? prep.context_id   ?? trigger.context_id ?? '');\nconst table_prefix = String(cur.table_prefix ?? merged.table_prefix ?? prep.table_prefix ?? trigger.table_prefix ?? '');\nconst context      = (cur.context ?? merged.context ?? prep.context ?? trigger.context ?? null);\n\n// Build combined message\nconst MAX = 1000;\nconst SUFFIX = ' ... [truncated] Message Summary: ';\nconst finalMessage = original.slice(0, MAX) + SUFFIX + summary;\n\n// Output with message_original populated\nreturn {\n  json: {\n    context_id,\n    table_prefix,\n    context,\n    role,\n    message_original: original,\n    message: finalMessage,\n  }\n};\n"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -880,
        144
      ],
      "id": "522df475-f90e-4a8c-bb60-b8b83253fbe5",
      "name": "Combine Message with Summary"
    },
    {
      "parameters": {
        "mode": "runOnceForEachItem",
        "jsCode": "// Message doesn't need summary, use original as-is\n// Pass through all context data\nreturn {\n  json: {\n    context_id: $json.context_id,\n    table_prefix: $json.table_prefix,\n    context: $json.context,\n    role: $json.role,\n    message_original: $json.message_original,\n    message: $json.message\n  }\n};"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -880,
        352
      ],
      "id": "56331973-7040-4a2a-8e67-acff4bf3f1ab",
      "name": "Use Original Message"
    },
    {
      "parameters": {},
      "type": "n8n-nodes-base.merge",
      "typeVersion": 2.1,
      "position": [
        -672,
        336
      ],
      "id": "cb40cad8-6df1-43d7-bc15-f9e83ba0c60c",
      "name": "Merge Messages"
    },
    {
      "parameters": {
        "model": {
          "__rl": true,
          "value": "gpt-4o-mini",
          "mode": "list",
          "cachedResultName": "gpt-4o-mini"
        },
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "typeVersion": 1.2,
      "position": [
        -1952,
        560
      ],
      "id": "1a5282f0-68ce-489f-bb21-236bf6256425",
      "name": "OpenAI Chat Model1",
      "credentials": {
        "openAiApi": {
          "id": "ypHiVnbB3jkc0yAx",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "model": {
          "__rl": true,
          "value": "gpt-4o-mini",
          "mode": "list",
          "cachedResultName": "gpt-4o-mini"
        },
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "typeVersion": 1.2,
      "position": [
        -1184,
        288
      ],
      "id": "70080ec7-9115-4d09-a146-76ec7e26eb50",
      "name": "OpenAI Chat Model2",
      "credentials": {
        "openAiApi": {
          "id": "ypHiVnbB3jkc0yAx",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "inputSource": "jsonExample",
        "jsonExample": "{\n  \"context_id\": \"823f85559ab443eeabb3887ead1db47b\",\n  \"table_prefix\": \"ura_dev_\",\n  \"userMessage\": \"I like fishing\",\n  \"assistantResponse\": \"That's great! Fishing is a wonderful hobby...\",\n  \"context\": \"{\\\"recent_messages\\\":[{\\\"id\\\":1,\\\"role\\\":\\\"user\\\",\\\"content\\\":\\\"hello\\\",\\\"was_truncated\\\":false},{\\\"id\\\":2,\\\"role\\\":\\\"assistant\\\",\\\"content\\\":\\\"hi there\\\",\\\"was_truncated\\\":false}],\\\"long_term_memories\\\":[{\\\"type\\\":\\\"preference\\\",\\\"content\\\":\\\"User likes fishing\\\"}]}\"\n}"
      },
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1.1,
      "position": [
        -2160,
        336
      ],
      "id": "adb5545b-b918-435e-8d2a-e3450f737de8",
      "name": "Start"
    },
    {
      "parameters": {
        "content": "Fixed Message Processing:\n- Messages > 5000 chars get truncated + AI summary\n- Original stored in message_original\n- Processed version in message column\n- Context data properly passed through all nodes\n- Fixed item pairing issues after merge\n\nENHANCEMENT:\n- Each message in context now includes database ID\n- Added 'was_truncated' flag to indicate if full message is in DB\n- Agent can retrieve original messages using ID when needed",
        "height": 240,
        "width": 350
      },
      "type": "n8n-nodes-base.stickyNote",
      "position": [
        -1600,
        16
      ],
      "typeVersion": 1,
      "id": "cfdb0ef1-94d4-4d1d-93eb-098ba1ca1e08",
      "name": "Sticky Note"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO {{ $('Start').first().json.table_prefix }}chat_transcript\n(context_id, role, message, message_original) VALUES (\n  '{{ String( $('Start').first().json.context_id).replace(/'/g, \"''\") }}',\n  '{{ String($json.role).replace(/'/g, \"''\") }}',\n  '{{ String($json.message).replace(/'/g, \"''\") }}',\n  '{{ String($json.message_original).replace(/'/g, \"''\") }}'\n);\n",
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        -480,
        336
      ],
      "id": "50e81693-b009-4db6-84b2-1b736bdd2f9a",
      "name": "Insert Transcript",
      "alwaysOutputData": true,
      "credentials": {
        "postgres": {
          "id": "p1yb7OdsllaTAj7J",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "SELECT\n  id, context_id, role, message, message_original, created_at, updated_at\nFROM {{ $('Start').first().json.table_prefix }}chat_transcript\nWHERE context_id = '{{ $('Start').first().json.context_id }}'\nORDER BY created_at DESC\nLIMIT 20;",
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        -288,
        240
      ],
      "id": "e99908a1-140b-4e69-9113-5a94244f3ddc",
      "name": "Fetch Transcript",
      "alwaysOutputData": true,
      "credentials": {
        "postgres": {
          "id": "p1yb7OdsllaTAj7J",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "SELECT\n  id,\n  topic_title,\n  topic_summary,\n  first_message_id,\n  last_message_id,\n  message_count,\n  created_at\nFROM {{ $('Start').first().json.table_prefix }}chat_topic_summaries\nWHERE context_id = '{{ $('Start').first().json.context_id }}'\n  AND last_message_id < (\n    SELECT MIN(id) FROM (\n      SELECT id FROM {{ $('Start').first().json.table_prefix }}chat_transcript\n      WHERE context_id = '{{ $('Start').first().json.context_id }}'\n      ORDER BY created_at DESC\n      LIMIT 10\n    ) recent\n  )\nORDER BY last_message_id DESC\nLIMIT 10;",
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        -64,
        240
      ],
      "id": "874a1a59-7722-4ee6-a6d2-52d1ec349c34",
      "name": "Fetch Topic Summaries",
      "alwaysOutputData": true,
      "credentials": {
        "postgres": {
          "id": "p1yb7OdsllaTAj7J",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO {{ $('Start').first().json.table_prefix }}chat_contexts\n(context_id, context)\nVALUES (\n  '{{ $json.context_id }}',\n  COALESCE(\n    NULLIF($$\n      {{ typeof $json.context === 'string'\n          ? $json.context\n          : JSON.stringify($json.context || {}) }}\n    $$, '')::jsonb,\n    '{}'::jsonb\n  )\n)\nON CONFLICT (context_id) DO UPDATE SET\n  context    = EXCLUDED.context,\n  \"updated_at\" = timezone('utc'::text, now());\n",
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        320,
        240
      ],
      "id": "b52d5766-6b51-41fa-880b-981d08c9509f",
      "name": "Upsert Context",
      "alwaysOutputData": true,
      "credentials": {
        "postgres": {
          "id": "p1yb7OdsllaTAj7J",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "SELECT id, role, message, created_at\nFROM {{ $('Start').first().json.table_prefix }}chat_transcript\nWHERE context_id = '{{ $('Start').first().json.context_id }}'\nORDER BY created_at DESC\nLIMIT 5;",
        "options": {
          "queryReplacement": ""
        }
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        -288,
        464
      ],
      "id": "f80d03df-0505-433d-b364-225a4b67050a",
      "name": "Fetch Recent for Detection",
      "alwaysOutputData": true,
      "credentials": {
        "postgres": {
          "id": "p1yb7OdsllaTAj7J",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "={{ $json.detection_prompt }}",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.agent",
      "typeVersion": 2.2,
      "position": [
        128,
        464
      ],
      "id": "065b1f2c-68b0-4204-ad1e-bb792e0e8566",
      "name": "Detect Topic Shift"
    },
    {
      "parameters": {
        "jsCode": "// Process topic detection result\nconst trigger = $('Start').first().json || {};\nconst aiOutput = $json.output || '{}';\n\n// Parse AI response\nlet result = { is_topic_shift: false, confidence: 0, new_topic_name: null, reasoning: '' };\ntry {\n  const cleaned = aiOutput\n    .replace(/```json\\n?/g, '')\n    .replace(/```\\n?/g, '')\n    .trim();\n  result = JSON.parse(cleaned);\n} catch (e) {\n  console.log('Failed to parse AI response:', aiOutput);\n}\n\nconst CONFIDENCE_THRESHOLD = 0.7;\nconst isShift = result.is_topic_shift && result.confidence >= CONFIDENCE_THRESHOLD;\n\n// Log the result\nconsole.log('=== TOPIC DETECTION ===' );\nconsole.log('Context ID:', trigger.context_id);\nconsole.log('Topic Shift:', isShift);\nconsole.log('Confidence:', result.confidence);\nconsole.log('New Topic:', result.new_topic_name);\nconsole.log('Reasoning:', result.reasoning);\nif (isShift) {\n  console.log('⚠️  TOPIC CHANGED - State will be reset');\n} else {\n  console.log('✓ Continuing current topic');\n}\nconsole.log('=======================');\n\n// Pass through for state update\nreturn [{\n  json: {\n    context_id: trigger.context_id,\n    topic_shift_detected: isShift,\n    shift_confidence: result.confidence,\n    suggested_topic_name: result.new_topic_name || 'Untitled Topic',\n    shift_reasoning: result.reasoning\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        416,
        464
      ],
      "id": "06ab4a1d-f666-4153-a3af-1d3ae48d1843",
      "name": "Log Detection Result"
    },
    {
      "parameters": {
        "model": {
          "__rl": true,
          "value": "gpt-4.1-mini",
          "mode": "list",
          "cachedResultName": "gpt-4.1-mini"
        },
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "typeVersion": 1.2,
      "position": [
        128,
        672
      ],
      "id": "7e1ffa30-adbf-4a6b-9881-19edf9b86878",
      "name": "OpenAI Chat Model",
      "credentials": {
        "openAiApi": {
          "id": "ypHiVnbB3jkc0yAx",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Combine all data for a SINGLE detection call\nconst trigger = $('Start').first().json || {};\nconst recentMessages = $input.all().map(i => i.json || {});\n\n// Format recent messages (excluding the new ones we just added)\nconst contextMessages = recentMessages\n  .filter(m => m.id) // has ID = already in DB\n  .sort((a, b) => a.id - b.id) // oldest to newest\n  .slice(0, -2) // exclude last 2 (the ones we just inserted)\n  .map(m => `${m.role}: ${m.message}`)\n  .join('\\n');\n\n// The NEW exchange we're analyzing\nconst newUserMsg = trigger.userMessage || '';\nconst newAssistantMsg = trigger.assistantResponse || '';\n\n// Build prompt\nconst prompt = `Analyze if this new exchange is a topic shift from previous conversation.\n\nPrevious context:\n${contextMessages || 'No previous messages'}\n\nNew exchange to analyze:\nuser: ${newUserMsg}\nassistant: ${newAssistantMsg}\n\nDid the topic change? Respond ONLY with JSON:\n{\n  \"is_topic_shift\": true or false,\n  \"confidence\": 0.0 to 1.0,\n  \"new_topic_name\": \"3-5 words\" or null,\n  \"reasoning\": \"one sentence\"\n}\n\nRules: shift = NEW subject. NOT shift = continuation of same subject.`;\n\nreturn [{\n  json: {\n    context_id: trigger.context_id,\n    detection_prompt: prompt\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        -64,
        464
      ],
      "id": "0cdebff9-a511-4c02-b80a-b117dbbd364f",
      "name": "Prepare Detection Data"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "-- Get current state before update\nWITH current_state AS (\n  SELECT \n    current_topic_start_id,\n    current_topic_name,\n    messages_in_current_topic\n  FROM {{ $('Start').first().json.table_prefix }}topic_state\n  WHERE context_id = '{{ $json.context_id }}'\n),\nlast_msg AS (\n  SELECT id FROM {{ $('Start').first().json.table_prefix }}chat_transcript\n  WHERE context_id = '{{ $json.context_id }}'\n  ORDER BY created_at DESC\n  LIMIT 1\n)\n-- Insert or update and return info\nINSERT INTO {{ $('Start').first().json.table_prefix }}topic_state\n(context_id, current_topic_start_id, current_topic_name, messages_in_current_topic, last_check_message_id)\nSELECT \n  '{{ $json.context_id }}',\n  CASE \n    WHEN {{ $json.topic_shift_detected }}::boolean THEN (SELECT id FROM last_msg)\n    ELSE COALESCE((SELECT current_topic_start_id FROM current_state), (SELECT id FROM last_msg))\n  END as new_start_id,\n  CASE \n    WHEN {{ $json.topic_shift_detected }}::boolean THEN '{{ String($json.suggested_topic_name || \"New Topic\").replace(/'/g, \"''\") }}'\n    ELSE COALESCE((SELECT current_topic_name FROM current_state), 'Initial Topic')\n  END as new_name,\n  CASE \n    WHEN {{ $json.topic_shift_detected }}::boolean THEN 1\n    ELSE COALESCE((SELECT messages_in_current_topic + 1 FROM current_state), 1)\n  END as new_count,\n  (SELECT id FROM last_msg)\nON CONFLICT (context_id) DO UPDATE SET\n  current_topic_start_id = EXCLUDED.current_topic_start_id,\n  current_topic_name = EXCLUDED.current_topic_name,\n  messages_in_current_topic = EXCLUDED.messages_in_current_topic,\n  last_check_message_id = EXCLUDED.last_check_message_id,\n  updated_at = timezone('utc'::text, now())\nRETURNING \n  (SELECT current_topic_start_id FROM current_state) as previous_topic_start_id,\n  (SELECT current_topic_name FROM current_state) as previous_topic_name,\n  (SELECT messages_in_current_topic FROM current_state) as previous_topic_message_count,\n  current_topic_start_id,\n  context_id,\n  {{ $json.topic_shift_detected }}::boolean as topic_shift_detected;",
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        608,
        464
      ],
      "id": "c7bc1ef8-638f-401e-a70a-6dd4cd249efe",
      "name": "Update Topic State",
      "alwaysOutputData": true,
      "credentials": {
        "postgres": {
          "id": "p1yb7OdsllaTAj7J",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "leftValue": "={{ $json.topic_shift_detected }}",
              "rightValue": true,
              "operator": {
                "type": "boolean",
                "operation": "equals"
              }
            },
            {
              "leftValue": "={{ $json.previous_topic_message_count }}",
              "rightValue": 3,
              "operator": {
                "type": "number",
                "operation": "gte"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        816,
        464
      ],
      "id": "b8518c9c-5841-44cf-ac3f-98a9da0af05e",
      "name": "Should Create Summary?"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "SELECT \n  id,\n  role,\n  message,\n  message_original,\n  created_at\nFROM {{ $('Start').first().json.table_prefix }}chat_transcript\nWHERE context_id = '{{ $json.context_id }}'\n  AND id >= {{ $json.previous_topic_start_id }}\n  AND id < {{ $json.current_topic_start_id }}\nORDER BY created_at ASC;",
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        1056,
        448
      ],
      "id": "fd942a8c-34db-41a8-8f6c-852a931a187c",
      "name": "Fetch Topic Messages",
      "alwaysOutputData": true,
      "credentials": {
        "postgres": {
          "id": "p1yb7OdsllaTAj7J",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Prepare messages for summarization\nconst stateData = $('Update Topic State').first().json;\nconst messages = $input.all().map(i => i.json);\n\n// Format conversation\nconst conversation = messages.map(m => {\n  const content = m.message_original || m.message;\n  return `${m.role}: ${content}`;\n}).join('\\n\\n');\n\n// Build summarization prompt\nconst prompt = `Create a concise summary of this conversation topic.\n\nConversation (${messages.length} messages):\n${conversation}\n\nProvide a JSON response (no markdown, no code blocks):\n{\n  \"topic_title\": \"Brief 5-8 word description\",\n  \"main_summary\": \"2-3 sentence overview\",\n  \"key_points\": [\"point 1\", \"point 2\", \"point 3\"],\n  \"decisions_made\": [\"decision 1\"] or [],\n  \"action_items\": [\"action 1\"] or []\n}\n\nGuidelines:\n- Keep concise and focused\n- Capture main ideas and important details\n- Omit greetings and casual chat`;\n\nreturn [{\n  json: {\n    context_id: stateData.context_id,\n    first_message_id: messages[0].id,\n    last_message_id: messages[messages.length - 1].id,\n    message_count: messages.length,\n    previous_topic_name: stateData.previous_topic_name,\n    summarization_prompt: prompt\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1264,
        448
      ],
      "id": "321e9b01-d21a-4976-8409-82498e281996",
      "name": "Prepare Summarization"
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "={{ $json.summarization_prompt }}",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.agent",
      "typeVersion": 2.2,
      "position": [
        1456,
        448
      ],
      "id": "944d71d5-0c64-4e8f-ab29-c60dc1fa4ca8",
      "name": "Generate Summary"
    },
    {
      "parameters": {
        "jsCode": "// Format the AI summary for storage\nconst summaryInput = $('Prepare Summarization').first().json;\nconst aiOutput = $json.output || '{}';\n\n// Parse AI response\nlet parsed;\ntry {\n  const cleaned = aiOutput\n    .replace(/```json\\n?/g, '')\n    .replace(/```\\n?/g, '')\n    .trim();\n  parsed = JSON.parse(cleaned);\n} catch (e) {\n  console.log('Failed to parse summary:', aiOutput);\n  parsed = {\n    topic_title: summaryInput.previous_topic_name || 'Topic',\n    main_summary: aiOutput.substring(0, 200),\n    key_points: [],\n    decisions_made: [],\n    action_items: []\n  };\n}\n\n// Format the full summary\nlet fullSummary = `${parsed.main_summary}\\n\\n`;\n\nif (parsed.key_points?.length > 0) {\n  fullSummary += `Key Points: ${parsed.key_points.join('; ')}\\n`;\n}\n\nif (parsed.decisions_made?.length > 0) {\n  fullSummary += `Decisions: ${parsed.decisions_made.join('; ')}\\n`;\n}\n\nif (parsed.action_items?.length > 0) {\n  fullSummary += `Actions: ${parsed.action_items.join('; ')}`;\n}\n\nconsole.log('=== TOPIC SUMMARY CREATED ===');\nconsole.log('Title:', parsed.topic_title);\nconsole.log('Messages:', summaryInput.first_message_id, 'to', summaryInput.last_message_id);\nconsole.log('Count:', summaryInput.message_count);\nconsole.log('============================');\n\nreturn [{\n  json: {\n    context_id: summaryInput.context_id,\n    topic_title: parsed.topic_title,\n    topic_summary: fullSummary.trim(),\n    first_message_id: summaryInput.first_message_id,\n    last_message_id: summaryInput.last_message_id,\n    message_count: summaryInput.message_count\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1760,
        448
      ],
      "id": "8e139c37-c34a-4520-b9aa-308d3196993f",
      "name": "Format Summary"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO {{ $('Start').first().json.table_prefix }}chat_topic_summaries\n(context_id, topic_title, topic_summary, first_message_id, last_message_id, message_count)\nVALUES (\n  '{{ $json.context_id }}',\n  '{{ String($json.topic_title).replace(/'/g, \"''\") }}',\n  '{{ String($json.topic_summary).replace(/'/g, \"''\") }}',\n  {{ $json.first_message_id }},\n  {{ $json.last_message_id }},\n  {{ $json.message_count }}\n);",
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        1952,
        448
      ],
      "id": "acd71c90-48e1-468c-8a7d-86d3fc22d0fc",
      "name": "Insert Summary",
      "alwaysOutputData": true,
      "credentials": {
        "postgres": {
          "id": "p1yb7OdsllaTAj7J",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "model": {
          "__rl": true,
          "mode": "list",
          "value": "gpt-4.1-mini"
        },
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "typeVersion": 1.2,
      "position": [
        1456,
        608
      ],
      "id": "c62c0d22-49f5-431f-9751-c0077eb6195a",
      "name": "OpenAI Chat Model3",
      "credentials": {
        "openAiApi": {
          "id": "ypHiVnbB3jkc0yAx",
          "name": "OpenAi account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Check for gap between summaries and recent messages\nconst trigger = $('Start').first().json || {};\nconst summaryRows = $('Fetch Topic Summaries').all().map(i => i.json || {});\nconst transcriptRows = $('Fetch Transcript').all().map(i => i.json || {});\n\n// Sort transcript by ID to find oldest recent message\nconst sortedTranscript = transcriptRows\n  .filter(r => r.id)\n  .sort((a, b) => a.id - b.id);\n\nconst oldestRecentMessageId = sortedTranscript.length > 0 ? sortedTranscript[0].id : 999999;\n\n// Find last summary's end message ID\nconst lastSummaryEndId = summaryRows.length > 0 \n  ? Math.max(...summaryRows.map(s => s.last_message_id || 0))\n  : 0;\n\n// Calculate gap\nconst gapStart = lastSummaryEndId + 1;\nconst gapEnd = oldestRecentMessageId - 1;\nconst gapSize = gapEnd - gapStart + 1;\nconst hasGap = gapSize >= 3 && lastSummaryEndId > 0;\n\nconsole.log('=== GAP DETECTION ===');\nconsole.log('Last summary ended at:', lastSummaryEndId);\nconsole.log('Recent window starts at:', oldestRecentMessageId);\nconsole.log('Gap:', gapStart, 'to', gapEnd, '=', gapSize, 'messages');\nconsole.log('Should fill gap:', hasGap);\nconsole.log('====================');\n\nreturn [{\n  json: {\n    context_id: trigger.context_id,\n    has_gap: hasGap,\n    gap_start_id: gapStart,\n    gap_end_id: gapEnd,\n    gap_size: gapSize,\n    summary_rows: summaryRows,\n    transcript_rows: transcriptRows\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1216,
        240
      ],
      "id": "7b4aa6e4-524d-47ab-bbf4-cff8ea7634c7",
      "name": "Detect Gap"
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "leftValue": "={{ $json.has_gap }}",
              "rightValue": true,
              "operator": {
                "type": "boolean",
                "operation": "equals"
              }
            }
          ],
          "combinator": "and"
        },
        "options": {}
      },
      "type": "n8n-nodes-base.if",
      "typeVersion": 2,
      "position": [
        1408,
        240
      ],
      "id": "2b488d5b-256b-444f-a326-196b14889529",
      "name": "Should Fill Gap?"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "SELECT \n  id,\n  role,\n  message,\n  message_original,\n  created_at\nFROM {{ $('Start').first().json.table_prefix }}chat_transcript\nWHERE context_id = '{{ $json.context_id }}'\n  AND id >= {{ $json.gap_start_id }}\n  AND id <= {{ $json.gap_end_id }}\nORDER BY created_at ASC;",
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        1632,
        32
      ],
      "id": "f793f693-48a3-4757-afd6-fca704d50b48",
      "name": "Fetch Gap Messages",
      "alwaysOutputData": true,
      "credentials": {
        "postgres": {
          "id": "p1yb7OdsllaTAj7J",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Prepare gap messages for summarization\nconst gapData = $('Should Fill Gap?').first().json;\nconst messages = $input.all().map(i => i.json);\n\n// Format conversation\nconst conversation = messages.map(m => {\n  const content = m.message_original || m.message;\n  return `${m.role}: ${content}`;\n}).join('\\n\\n');\n\n// Build prompt\nconst prompt = `Create a concise summary of this conversation segment.\n\nConversation (${messages.length} messages):\n${conversation}\n\nProvide JSON (no markdown):\n{\n  \"topic_title\": \"5-8 word description\",\n  \"main_summary\": \"2-3 sentences\",\n  \"key_points\": [\"point 1\", \"point 2\"],\n  \"decisions_made\": [],\n  \"action_items\": []\n}\n\nKeep concise and capture main ideas.`;\n\nconsole.log('=== FILLING GAP ===');\nconsole.log('Gap messages:', gapData.gap_start_id, 'to', gapData.gap_end_id);\nconsole.log('Count:', messages.length);\nconsole.log('===================');\n\nreturn [{\n  json: {\n    context_id: gapData.context_id,\n    first_message_id: messages[0].id,\n    last_message_id: messages[messages.length - 1].id,\n    message_count: messages.length,\n    summarization_prompt: prompt,\n    gap_data: gapData\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1808,
        32
      ],
      "id": "6f33db1f-0b82-4ab4-b68e-8f1b3b7bc1b4",
      "name": "Prepare Gap Summary"
    },
    {
      "parameters": {
        "promptType": "define",
        "text": "={{ $json.summarization_prompt }}",
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.agent",
      "typeVersion": 2.2,
      "position": [
        2016,
        32
      ],
      "id": "1d06ca8d-be9e-4d5a-8ff9-c5ecb0493ccd",
      "name": "Generate Gap Summary"
    },
    {
      "parameters": {
        "jsCode": "// Format gap summary for storage\nconst summaryInput = $('Prepare Gap Summary').first().json;\nconst aiOutput = $json.output || '{}';\n\n// Parse AI response\nlet parsed;\ntry {\n  const cleaned = aiOutput\n    .replace(/```json\\n?/g, '')\n    .replace(/```\\n?/g, '')\n    .trim();\n  parsed = JSON.parse(cleaned);\n} catch (e) {\n  console.log('Failed to parse gap summary:', aiOutput);\n  parsed = {\n    topic_title: 'Gap Summary',\n    main_summary: aiOutput.substring(0, 200),\n    key_points: []\n  };\n}\n\n// Format full summary\nlet fullSummary = `${parsed.main_summary}\\n\\n`;\nif (parsed.key_points?.length > 0) {\n  fullSummary += `Key Points: ${parsed.key_points.join('; ')}`;\n}\n\nconsole.log('=== GAP SUMMARY CREATED ===');\nconsole.log('Title:', parsed.topic_title);\nconsole.log('Messages:', summaryInput.first_message_id, 'to', summaryInput.last_message_id);\nconsole.log('===========================');\n\nreturn [{\n  json: {\n    context_id: summaryInput.context_id,\n    topic_title: parsed.topic_title,\n    topic_summary: fullSummary.trim(),\n    first_message_id: summaryInput.first_message_id,\n    last_message_id: summaryInput.last_message_id,\n    message_count: summaryInput.message_count,\n    gap_data: summaryInput.gap_data\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2384,
        32
      ],
      "id": "a2c0dea7-2672-4478-bb9e-959a1b116627",
      "name": "Format Gap Summary"
    },
    {
      "parameters": {
        "operation": "executeQuery",
        "query": "INSERT INTO {{ $('Start').first().json.table_prefix }}chat_topic_summaries\n(context_id, topic_title, topic_summary, first_message_id, last_message_id, message_count)\nVALUES (\n  '{{ $json.context_id }}',\n  '{{ String($json.topic_title).replace(/'/g, \"''\") }}',\n  '{{ String($json.topic_summary).replace(/'/g, \"''\") }}',\n  {{ $json.first_message_id }},\n  {{ $json.last_message_id }},\n  {{ $json.message_count }}\n);",
        "options": {}
      },
      "type": "n8n-nodes-base.postgres",
      "typeVersion": 2.6,
      "position": [
        2592,
        32
      ],
      "id": "8143767a-f1ef-431e-a3ae-db2df6b329e9",
      "name": "Insert Gap Summary",
      "alwaysOutputData": true,
      "credentials": {
        "postgres": {
          "id": "p1yb7OdsllaTAj7J",
          "name": "Postgres account"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Merge summaries - handle both paths (gap filled or not)\nconst inputData = $input.first().json;\n\n// Determine which path we came from\nlet summaryRows = [];\nlet transcriptRows = [];\n\nif (inputData.gap_data) {\n  // Came from gap-filling path - has gap_data passed through\n  summaryRows = inputData.gap_data.summary_rows || [];\n  transcriptRows = inputData.gap_data.transcript_rows || [];\n  \n  // Add the newly created gap summary\n  summaryRows.push({\n    topic_title: inputData.topic_title,\n    topic_summary: inputData.topic_summary,\n    first_message_id: inputData.first_message_id,\n    last_message_id: inputData.last_message_id,\n    message_count: inputData.message_count\n  });\n  \n  // Re-sort by message range\n  summaryRows.sort((a, b) => a.first_message_id - b.first_message_id);\n  \n} else {\n  // Came from no-gap path - use data as-is\n  summaryRows = inputData.summary_rows || [];\n  transcriptRows = inputData.transcript_rows || [];\n}\n\nreturn [{\n  json: {\n    summary_rows: summaryRows,\n    transcript_rows: transcriptRows\n  }\n}];"
      },
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        2768,
        256
      ],
      "id": "d84b6262-46e2-47fb-9eb4-1a1adcafc756",
      "name": "Merge Gap Summary"
    },
    {
      "parameters": {
        "model": {
          "__rl": true,
          "mode": "list",
          "value": "gpt-4.1-mini"
        },
        "options": {}
      },
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenAi",
      "typeVersion": 1.2,
      "position": [
        2016,
        176
      ],
      "id": "490e7f22-cef3-4ddf-b63c-d840f9c407ae",
      "name": "OpenAI Chat Model4",
      "credentials": {
        "openAiApi": {
          "id": "ypHiVnbB3jkc0yAx",
          "name": "OpenAi account"
        }
      }
    }
  ],
  "pinData": {},
  "connections": {
    "Memory Manager": {
      "main": [
        [
          {
            "node": "Prepare Messages",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Chat Model1": {
      "ai_languageModel": [
        [
          {
            "node": "Memory Manager",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Chat Model2": {
      "ai_languageModel": [
        [
          {
            "node": "Summarize Long Message",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Start": {
      "main": [
        [
          {
            "node": "Memory Manager",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Messages": {
      "main": [
        [
          {
            "node": "Check If Summary Needed",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Check If Summary Needed": {
      "main": [
        [
          {
            "node": "Summarize Long Message",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Use Original Message",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Summarize Long Message": {
      "main": [
        [
          {
            "node": "Combine Message with Summary",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Combine Message with Summary": {
      "main": [
        [
          {
            "node": "Merge Messages",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Use Original Message": {
      "main": [
        [
          {
            "node": "Merge Messages",
            "type": "main",
            "index": 1
          }
        ]
      ]
    },
    "Merge Messages": {
      "main": [
        [
          {
            "node": "Insert Transcript",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build Next Context": {
      "main": [
        [
          {
            "node": "Upsert Context",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Insert Transcript": {
      "main": [
        [
          {
            "node": "Fetch Transcript",
            "type": "main",
            "index": 0
          },
          {
            "node": "Fetch Recent for Detection",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Fetch Transcript": {
      "main": [
        [
          {
            "node": "Fetch Topic Summaries",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Fetch Topic Summaries": {
      "main": [
        [
          {
            "node": "Detect Gap",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Fetch Recent for Detection": {
      "main": [
        [
          {
            "node": "Prepare Detection Data",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Detect Topic Shift": {
      "main": [
        [
          {
            "node": "Log Detection Result",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Chat Model": {
      "ai_languageModel": [
        [
          {
            "node": "Detect Topic Shift",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Detection Data": {
      "main": [
        [
          {
            "node": "Detect Topic Shift",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Log Detection Result": {
      "main": [
        [
          {
            "node": "Update Topic State",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Update Topic State": {
      "main": [
        [
          {
            "node": "Should Create Summary?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Should Create Summary?": {
      "main": [
        [
          {
            "node": "Fetch Topic Messages",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Fetch Topic Messages": {
      "main": [
        [
          {
            "node": "Prepare Summarization",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Summarization": {
      "main": [
        [
          {
            "node": "Generate Summary",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Generate Summary": {
      "main": [
        [
          {
            "node": "Format Summary",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format Summary": {
      "main": [
        [
          {
            "node": "Insert Summary",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Chat Model3": {
      "ai_languageModel": [
        [
          {
            "node": "Generate Summary",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Detect Gap": {
      "main": [
        [
          {
            "node": "Should Fill Gap?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Should Fill Gap?": {
      "main": [
        [
          {
            "node": "Fetch Gap Messages",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Merge Gap Summary",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Fetch Gap Messages": {
      "main": [
        [
          {
            "node": "Prepare Gap Summary",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Gap Summary": {
      "main": [
        [
          {
            "node": "Generate Gap Summary",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Generate Gap Summary": {
      "main": [
        [
          {
            "node": "Format Gap Summary",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Format Gap Summary": {
      "main": [
        [
          {
            "node": "Insert Gap Summary",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Insert Gap Summary": {
      "main": [
        [
          {
            "node": "Merge Gap Summary",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge Gap Summary": {
      "main": [
        [
          {
            "node": "Build Next Context",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "OpenAI Chat Model4": {
      "ai_languageModel": [
        [
          {
            "node": "Generate Gap Summary",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "71035265-4618-48de-ab54-a35b2e8c0717",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "7ac72ab13b8f564b4e52fe865fd322b93ff4a198e7d01a343aa643a3d940b098"
  },
  "id": "e7XF3f8ZZCeSZyw0",
  "tags": []
}